# 2026-02-08 — Day 2: Collision Detection & 3D Calibration

## Major Accomplishments

### PRs Merged
- **PR #6 (Claw Position Prediction)** — Resolved merge conflicts, adapted from stereo to independent camera architecture, merged to main
- **PR #7 (Vision Task Planning)** — CI fix (removed unused google imports), merged to main

### Camera Infrastructure
- Fixed camera server crashes — updated watchdog to manage both camera servers (port 8081)
- Camera URLs confirmed: `localhost:8081/snap/0` and `/snap/1`
- Cameras are independent (not stereo), max ~1080p each

### Collision Detection System (New)
- Built `collision_detector.py` — checks proposed moves against known collision zones
- Built `collision_analyzer.py` — vision-based analysis of collision events
- Integrated collision alerts into web UI (compact toast in bottom-right, not banner)
- Added collision memory — arm learns where it hits and avoids those zones
- Collision data files gitignored
- **Issue:** ~70% of collision alerts during calibration were false positives from stale joint feedback

### 3D Calibration System Rewrite
- Ran a council of 3 specialist agents for calibration analysis
- Reset viz calibration to DH-derived proportions (121.5/208.5/208.5/113mm link lengths)
- Rewrote `viz_calibrator.py` from scratch:
  - Proper DH-based forward kinematics
  - Pinhole camera projection model
  - Frame differencing for matte-black arm detection
  - Gold/yellow segment detection via HSV (H:20-40)
  - Dual independent camera support
  - `scipy.optimize.least_squares` solver
  - Freeze link lengths to DH values, only optimize offsets + camera extrinsics
- Updated `server.py` endpoint (fixed scipy vs cv2 Rodrigues issue)
- Updated JS loader for new calibration format
- Tuned solver convergence (reduced max_nfev, added ftol/xtol/gtol)

### Testing
- All 460 tests passing
- CI green

## Lessons Learned

### Hardware Control
- **Use `set_joint()` not `set_all_joints()`** — the latter causes arm freezes on large simultaneous moves
- **5° margin from hardware limits** on J1/J2/J4 — never command beyond ±85°
- **Small increments ≤10° per command** — verify feedback before next move
- **Go home when stuck** — always return to home position when jammed

### Vision & Calibration
- **Vision models unreliable for distance measurement** — good for detection, terrible for quantitative measurement
- **Frame differencing beats HSV** for detecting matte-black arm against arbitrary backgrounds
- **Gold/yellow arm segments** detectable via HSV H:20-40
- **Lock J0=0° during calibration** — simplifies the optimization problem
- **Freeze link lengths to DH values** — only optimize joint offsets + camera transform (6 params per camera)
- **Cameras are independent**, not stereo — no overlap, different viewpoints (overhead + front/side)

### Collision Detection
- **~70% false positive rate** from stale joint feedback — feedback lags behind commands
- Need continuous monitoring, not just post-move checks
- Collision memory is useful but needs better data quality first

### Process
- Council of specialist agents works well for analysis tasks
- Progressive calibration (self-assessment) was attempted but solver convergence was the real issue

## Mistakes Made
1. Initially tried stereo calibration — cameras aren't a stereo pair
2. Used cv2.Rodrigues in server endpoint — scipy environment didn't have cv2, switched to scipy
3. Solver hung with default parameters — needed explicit convergence limits
4. Collision alerts as full-width banner were too intrusive — switched to toast

## Commits Today (chronological)
- Camera calibration data, ruler-based calibration
- Watchdog fix for camera server
- Claw position prediction merge (PR #6)
- Vision task planning merge (PR #7)
- Vision-assisted collision detection
- Collision alert UI (toast style)
- Viz calibration reset to DH proportions
- 3D calibration rewrite (DH FK, pinhole, frame diff, dual camera)
- Solver convergence tuning
- Scipy Rodrigues fix
- Collision memory (learning from hits)

## Goals for Tomorrow (2026-02-09)

### Primary
1. **First real hardware calibration run** — connect arm, run `viz_calibrator.py`, target <10px residual
2. **Validate end-to-end viz accuracy** — move arm to known positions, verify 3D overlay matches reality
3. **Fix collision false positives** — implement feedback freshness check (reject stale data >100ms)

### Secondary
4. Add continuous collision monitoring (background thread, not just post-move)
5. Add J0/J3/J5/gripper support to calibration
6. Start on camera auto-discovery/health monitoring

### Hardware Setup Needed
- Confirm D1 is powered and network-reachable
- Verify both cameras streaming on port 8081
- Clear workspace around arm for calibration movements
- Have calibration target (checkerboard or known object) ready
